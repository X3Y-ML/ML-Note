什么是最优化？

[【知乎】最优化问题的简洁介绍是什么？](https://www.zhihu.com/question/26341871)

**最优化需要精心考虑的问题：**
+ 合理的目标函数
+ 如何求解目标函数


> 机器学习的目的，就是为了让计算机代替人来发现数据之间隐藏的关系。

> 研究者建立很多学习模型，这些模型输入是一个样本的数据（头像图片、一个人的DNA序列），输出是样本的标签人名、是否患病）。模型里有大量可以调整的参数，这些参数通过训练，能够学习到数据和标签之间人类无法直接理解的、复杂的关系。科学家期望当模型训练完成后，再拿来一个样本，喂给这个训练好的机器，它能够吐出一个标签，这个标签恰好就是样本对应的那个正确的标签。

> 目前人们已经研究出一大堆学习模型：神经网络、支持向量机、AdaBoost、随机森林、隐马尔科夫
链、卷积神经网络等等。它们的结构差异很大，但是共同点都是拥有一大堆参数，就等着你喂给它数据供它学习。这些模型的学习也需要一个目标函数：
<b>让模型的分类错误率尽量小</b>
。为了达到目的，模型的训练往往首先给参数赋上随机初值，然后用各种下降法来寻找能让分类错误率更小的参数设置，梯度下降、牛顿法、共轭梯度法和Levenberg—Marquard法都是常见的方法

> 随着研究的深入，问题也越来越多，比如下降法往往只能保证找到目标函数的局部最小值，找不到全局最小值，怎么办呢？答案是不一味下降、也适当爬爬山，说不定能跳出小水沟（局部极小值）找到真正的深井（全局极小值），这种算法叫模拟退火。也可以增大搜索范围，让一群蚂蚁（蚁群算法）或者鸟儿（粒子群算法）一齐搜索，或者让参数巧妙地随机改变（遗传算法）。

> 那么多模型，到底该选哪个？研究者又发现了一个定理“天下没有免费的午餐”定理，意思是没有一个模型能一直比其他模型好，对于不同类型的数据，必须要通过实验才能发现哪种学习模型更适合。机器学习领域也就成了学界灌水严重的领域之一——换模型、调参数就能发文章哎。

> 下面说到了调参数，问题又来了，到底是参数多了好还是少了好？参数少了模型太笨学不到数据内的复杂关系，参数多了模型太精明又可能会把数据中的随机噪声当作某种关系进行认真学习（过拟合）。最后大家一致认为，确定模型的复杂度时，要保证模型能力足够强，能够学会数据之间的关系，能力又不能太强，以至于耍小聪明乱学习。这种选择模型的思想被称为**奥卡姆剃刀**：选择有能力的模型中最简单的那个。此外，训练模型的目标并不是为了使训练样本能够被尽量正确分类，更需要对未知新样本有好的分类效果，这样模型才有实用价值，这种能力被称为**泛化能力**。除了奥卡姆剃刀原理外，训练时引入随机性的模型比确定的模型（比如BP神经网络）具有更好的泛化能力。

> 模型的更新也是问题。如果引入了新数据，全部模型都需要重新训练是一笔很大的开销，在线学习模型采用来一个样本学一点的模式，能够不断自我更新；半监督学习利用少量带标签的样本训练一个原始模型，然后利用大量无标签数据再学习。

> 模型的更新也是问题。如果引入了新数据，全部模型都需要重新训练是一笔很大的开销，在线学习模型采用来一个样本学一点的模式，能够不断自我更新；半监督学习利用少量带标签的样本训练一个原始模型，然后利用大量无标签数据再学习。

![ss](https://pic1.zhimg.com/50/9cb3e816be0669433f2e52a15e5848c0_hd.jpg)

> 下面是支持向量机的分类结果，这是这几十年机器学习最重要的成果之一，它的发明是基于结构最小化准则，通俗地讲就是把目标函数设为：J=模型分类正确率 + r * 模型复杂度
使得模型能够自动选择分类效果好，并且尽量简单的参数。

![ss](https://pic3.zhimg.com/50/c85dbbaa308b2fe26c0d9369cb231ce6_hd.jpg)

>  接下来是随机树，它把空间划分为一系列矩形区域（叶子），所有的叶子区域由一颗树形结构从根节点不断划分而成，随机的意思是树的生长每次划分第一维还是第二维是随机的：

![](https://pic3.zhimg.com/50/8836979bc3c077c5569d8c7f43aeaa12_hd.jpg)

> 支持向量机对于小样本数据和非线性结构数据的分类有十分优秀的表现

> 在机器学习领域，还有很多重要问题被不断讨论，优秀的模型也不断在涌现。这个领域的开山模型是神经元，由其组成的多层神经网络由于训练速度慢、分类效果不佳，在支持向量机出现后很快就失去了热度。大家卯着劲研究怎么面对训练样本不足的窘境，PCA和核方法大行其道，前者致力于减少数据维数，后者致力于从低维数据中学习高维结构。但是近几年随着卷积神经网络的流行，神经网络又焕发出了第二春，研究者发现只要样本量足够大（百万级甚至亿级样本量），网络参数足够多（百万级参数），加上巧妙的防过拟合技术，利用现代并行计算带来的强大计算能力，神经网络能够学得和人类的判别能力一样好。机器学习领域发展了几十年，似乎又回到了出发的地方。



>优化问题的核心有三部分，决策（Decision），目标（Objective）和约束（Constraint）
优化的目的是在选取一个（或一些决策），在满足一定约束情况下，尽可能达到某一目标。在去思考优化问题时，最好的顺寻就是问以下问题：
> 1. 我要做的决策是什么？
> 2. 我要达到的目标是什么？
>3. 我的决策有什么约束？

>事实上，所有的决策问题，小到我们生活中的每一个选择，大到国家的战略，都可以分解为这样的三部分（包括所有的机器学习问题）。因此优化的思想可以说是在人们的生活中无处不在，也是世间万物的一种基本规律。数学家欧拉早在18世纪就说过：Nothing at all takes place in the universe in which some rule of maximum or minimum does not appear. – L. Euler, 1707-1783在实际中，搞清楚实际问题的这三部分分别是什么，并且用合理的方式去表达是最解决问题中最终要的一步。完成了这一步（所谓的建模）通常已经完成了解决问题的绝大部分。后面就需要用到优化的算法。这两部分也是学习运筹优化的核心。
作者：王子卓
链接：https://www.zhihu.com/question/26341871/answer/175517606
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。


> 在机器学习领域有个普遍的观点：所有机器学习的问题最后都转换为了最优化问题。拟合一些数据点，你可以选择拟合，你也可以选择拟合，你甚至可以选择更高维的曲线拟合，那么选择哪个？ -- 最优化问题两堆数据点，你想在中间画一条直线，将其分开，实际上这样的直线很多，选择哪一个？最优化问题。任何一个机器学习问题，我们提一个模型来描述问题很简单，所有可能的模型组成了假设空间。那么机器学习最后都转化到 在假设空间中找到最优解。 求最优解的策略很多，选择经验风险最小化或结构风险最小化，最后都还是最优化问题。当然机器学习的内容肯定不止于此，数据、模型、算法都有很多内容值得探索。
作者：杨延生
链接：https://www.zhihu.com/question/26341871/answer/32541857
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。








































#
